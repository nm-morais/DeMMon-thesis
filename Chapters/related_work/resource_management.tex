
In this section, we study resource management in the context of edge environments. Resource management consists in providing resources (e.g. computing power, memory, among others) to tenants (i.e. applications, frameworks, among others), such that these can perform their computations. In this section, we cover aspects of resource management solutions and study popular solutions in the literature.

\subsection{Resource Management Taxonomy}

A resource management system aims at controlling the distribution of resources among tenants. We may classify resource management architectures according to their \textit{control} and \textit{tenancy}.

\subsubsection{Tenancy}

The term tenancy in resource management refers to whether or not underlying hardware resources are shared among entities \cite{Hong2019}.

\textbf{Single tenancy} refers to an architecture in which a single instance of a software application and supporting infrastructure serves one customer. In single-tenancy architectures, a customer (tenant) has nearly full control over the customization of software and infrastructure.

\textbf{Multi-tenancy} consists of tenants sharing multiple resources across multiple processes and machines. This approach has clear advantages, as sharing the infrastructure leads to lower costs (e.g. electricity), and companies of all sizes like to share infrastructure in order to achieve lower operational costs.

However, providing performance guarantees and isolation in multi-tenant systems is extremely hard, resource management systems must avoid mismatching the resource allocation, as tenant-generated requests compete with each other and with the system generated tasks. Furthermore, tenant workload can change in unpredictable ways depending on the input workload, the workload of other tenants in the system, and the underlying topology.

\subsubsection{Control}

Control refers to how resource management systems allocates tasks to available resources, there are two alternatives towards performing resource allocations: either \textit{centralized} or \textit{decentralized}.

\textbf{Centralized control} consists in a centralized component with a global view of the state of the system making all decisions regarding resource allocations. Intuitively, given that a centralized component generates manages all the resources in the system, this component can easily enforce policies to achieve the desired performance guarantees or fairness goals by identifying and only throttling the tenants or system activities responsible for resource bottlenecks \cite{verma2015large}.

\textbf{Decentralized control} architectures are defined by having the decision-making process regarding resource allocations distributed across multiple components \cite{Hong2019}. This topic has yet not been subject to much research, although it is of extreme relevance towards edge environments. For example, if the system is globally distributed, it may take too long for a centralized controller to identify hotspots in a certain zone and load-balance them.

One of the key challenges in distributed resource management is ensuring that the components which perform resource assignments do not conflict with each other. Additionally, in a multi-tenant decentralized resource management system, tenants may request resources to different resource controllers in the system, and if they do not coordinate themselves, the application may be provisioned with too many (or too little) resources.

\subsection{Resource Management Systems}

\textbf{Mesos} \cite{hindman2011mesos} is a multi-tenant centralized resource sharing platform that attempts to provide fine-grained resource sharing within a data centre. The tenants for this platform are frameworks such as HDFS \cite{borthakur2008hdfs}, MapReduce \cite{dean2008mapreduce}, among others, which in turn support multiple applications running within a DC. In short, the Mesos resource sharing system consists of a \textit{master} process which manages \textit{slave} daemons running on each cluster node. In order to achieve fault-tolerance for the master component, Mesos employs Zookeeper \cite{hunt2010zookeeper} to maintain replicas, elect a new master, and transfer state to a new master in case the active master fails.

The master implements fine-grained sharing of resources across frameworks by employing \textit{resource offers}, which consist of lists containing free resources distributed among slaves. The master makes decisions about how many resources to offer to each framework, and the decision-making process is based on an arbitrary organizational policy, such as fair sharing or priority. Each framework that wishes to use Mesos must implement a \textit{scheduler} and an \textit{executor}. The scheduler registers with the Mesos master to receive resource offers, and the executor is the process that is launched on slave nodes to run the framework's tasks.

A limitation of the Mesos resource sharing platform is that it is not scalable, given the central component issuing resource allocations (the original authors mention the system scales up to 50000 slave daemons on 99 physical machines), which is not enough for an edge environment. Furthermore, the resource offer model forces frameworks to employ a specific programming model based on schedulers and executors, which we believe to be too restrictive.

\textbf{Yarn} (Yet Another Resource Negotiator) \cite{Vavilapalli2013ApacheHY} is a centralized multi-tenant resource sharing platform that decouples the programming model from the resource management infrastructure and delegates many scheduling functions to per-application components. The architecture of YARN is composed by: a per-cluster Resource Manager (RM), multiple Application Masters (AM), and Node Managers (NM). The Resource Manager (RM) tracks resource usage and node liveness, enforces allocation invariants and arbitrates contention among tenants.

Application Masters (AM) run arbitrary user code, their duties in the system consist of managing the lifecycle aspects, including dynamically increasing and decreasing resource consumption, managing the flow of execution, and handling faults. Node Managers (NM) are worker daemons, whose responsibilities consist of managing container dependencies, monitoring their execution, and providing a set of services for them.

AMs send resource requests to the RM, containing the number of containers to request, the resources per container, locality preferences, and a priority level within the application. These requests are designed to capture the needs of applications while at the same time removing application concerns (such as task dependencies) from the scheduler. Because the RM is in charge of processing and scheduling all task distributions for each request made by AMs, it is effectively a \textit{monolithic} scheduler. By consequence, there is a unique point of failure, which makes this system inadequate for large scale edge environments.

\textbf{Omega} \cite{41684} is a scheduler designed for grid computing systems composed by schedulers and workers. Each scheduler receives large amounts of jobs composed by either one or many tasks that have to be scheduled among workers. Contrary to YARN, which is monolithic, OMEGA uses multiple schedulers per cluster, each with a shared global view of the cluster state.

Schedulers make task placement decisions according to their view of the cluster state and their scheduling policy. If two or more schedulers attempt to schedule a task to the the same worker (i.e., generating a conflict), the worker first tries to accommodate both tasks, if it cant, it rejects the least important one.

One advantage of OMEGA in relation to MESOS is that MESOS resource attributions ``lock'' the resources to the corresponding framework,  which means that only one framework is examining a resource at a time. While it achieves higher throughput in allocation operations, its main limitations are that: (1) in case the grid becomes overloaded, resource allocations can potentially start interfering with each other; (2) scheduling policies are harder to ensure; and finally, (3) all schedulers must have global knowledge of the system.

\textbf{Edge NOde Resource Management} \cite{wang2017enorm} (ENORM) is framework aimed at employing edge resources towards applications by provisioning and auto-scaling edge node resources. ENORM proposes a three-tier architecture: (1) the Cloud tier, where application servers are hosted; (2) the middle tier, where the edge nodes are situated; and (3) the bottom tier, where user devices (e.g. smartphones, wearables, gadgets) are situated.

To enable the use of edge nodes, ENORM deploys a cloud server manager on each application server, which communicates with potential edge nodes, requesting computing services. Using these computing resources, it deploys partitioned servers on the edge nodes. Edge nodes are maintained in a global view.

ENORM authors tested the designed system using an online game inspired on Pokemon GO (iPokemon)\cite{pokemonGo}. The ENORM framework partitions the game server and sends user data to each edge node containing information regarding the users within that geographical location. Users from the relevant geographical zone then connect to the edge server and are serviced by a geographically closer edge node as if they were connected to the data centre. Limitations from this framework are the large size of the required information to perform the deployments, and similarly to previous solutions, the lack of fault-tolerance and scalability, from employing a centralized component to perform monitoring and management of resources.

\textbf{FogTorch} \cite{Brogi2017} is a service deployment framework aimed at determining eligible deployments for an application over a given Fog infrastructure, modeled by: (1) Cloud Data Centers, denoted by their location and software capabilities; (2) Fog Nodes, that consist of tuples containing: the location, hardware, the software capabilities, and the things directly reachable from the fog node; (3) Things, which are represented by a tuple denoting the thing (sensor or actuator) location and its type; (4) QoS profiles, that are sets of QoS profiles composed by the latency and bandwidth of a communication link. (5) Applications, which are composed of independent sets of components, each with a set of requirements regarding QoS profiles, hardware and software capabilities, and things. Then, authors model service deployments as restrictions over the system model and employ a greedy heuristic, which reduces the search space of devices constituting options for these service deployments.

FogTorch is also the base for \textbf{FogTorchPI} \cite{brogi2017best}, which is a solution that employs the system model of FogTorch, however instead of a greedy approach, it uses Monte Carlo simulations to calculate the best possible deployment configurations.

These solutions provide a comprehensive system model which models many different types of application requirements, however,similarly to FogTorch, it requires an updated global view of the system, which requires collecting a large amount of information to a central entity, limiting system scalability.

\subsection{Discussion}

Although resource management systems have been present for many years, these are often tailored towards small scale environments composed by homogenous devices in stable environments, which contrast with the edge of the network, where devices are extremely numerous, operate on a decentralized fashion, and are highly heterogenous.

We argue that a centralized controller is not the ideal solution for an edge environment, given the fact that as the number of devices in the system increases, so does the number of resources to track, and the harder it is for a centralized component to have an up-to-date global view of the system.

Due to their low capacity, devices at the edge of the network are very susceptible to workload changes, for example, a 5G tower that is hosting services cannot handle a drastic increase in the number of users it is serving. In this scenario, we argue that in order to maintain pre-established performance criteria, devices must autonomously make resource management decisions such as scaling an allocation horizontally or vertically in order to quickly meet the demands of users/tenants.

% \subsubsection{Scheduling}

% There are many approaches towards scheduling resources among edge nodes: \textbf{Brute force} proposes exhaustively exploring all potential targets combinations towards offloading tasks (including the Cloud, the Edge and other user devices) and picking the one which provides the minimum execution time. This technique intuitively is not scalable enough to be applied in practice.

% \textbf{Greedy} heuristics focus on minimizing the time it takes for the task to be completed on a mobile device. FogTorch \cite{Brogi2017} employs a greedy heuristic by which reduces the search space of devices that constitute options for service deployments. 

% \textbf{Simulated Annealing} employs a search space based on the utilization of edge and cloud nodes, total costs, and the completion time of the task to find the optimal solution.

% \subsubsection{Offloading}

%In this section we study offloading, which is technique used by edge-enabled applications to fully take advantage of edge nodes.

%Offloading is a technique in which a server, application and the associated data are transferred onto another node in the network \cite{Hong2019}. There are two variants for offloading: either from the user device to the edge, or from the cloud to the edge. 

%Offloading from user device to the edge enhances computing in mobile nodes by employing edge nodes which are usually only one or two hops away. While offloading from the Cloud to the edge has the potential to reduce bandwidth consumption and improve QoS of edge-enabled applications. 

%\textbf{Server offloading} is a technique in which servers are offloaded to the edge via either replication or partitioning. \textbf{Replication} consists in offloading the full server state(e.g. a database or an  application server), while \textbf{partitioning} consists in offloading only a portion of the server state. 

%The portion of the server to offload must take into account a set of parameters such as latency, functionality, energy efficiency, geographical distribution, among others. 


%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% evaluation.tex
%% NOVA thesis document file
%%
%% Chapter with lots of dummy text
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE evaluation.tex}

\chapter{Evaluation}
\label{cha:evaluation}

In this chapter, we test the performance and applicability of the deMMon framework. It is our goal to demonstrate the applicability of the devised solution through the comparison of multiple aspects of the framework against popular solutions (in each aspect) from the literature. To this end, section \ref{sec:exp_setting_conf} covers the experimental setting and configuration, namely the specifications of the nodes executing the solutions, the methods used to control the experiment scenarios, among other aspects of the experimental setting. Following, in section \ref{sec:overlay_proto_eval}, the applicability of the overlay protocol is tested in two ways: the first test compares the characteristics of the resulting network against a set of baseline protocols, and the second overlay test evaluates the performance of the protocol in performing message dissemination against the same set of baseline protocols (executing dissemination protocols). Lastly, section \ref{sec:agg_proto_eval}, covers the experimental evaluation for aggregation protocol, notably, which solutions composed the baseline for comparison, which experiments were carried, and provides the obtained experimental results. All of the previously mentioned sections are finished with a discussion of the obtained results.

\section{Experimental Setting} \label{sec:exp_setting_conf}

To conduct the experimental evaluation of the devised solution, instead of resorting to simulation, solutions were implementated and tested in real-world scenarios. However: (1) as scalability is one of the components to be tested and there is a limited pool of individual machines in the testbed to conduct the experiemental evaluation; (2) in order to emulate a real-world scenario, there was the need to both limit the networking capacity and inject latency among nodes, we resorted to using containerization. Containers allowed us to to run multiple independent processes in a isolated environment, and allowed the manipulation of the networking capacity of each process. 

As containers are running in different machines, without any additional software, a container from a machine would not be able to communicate with containers executing in a different machines. To solve this, \todo{cite} we made use of docker containers, and employed a tool called docker swarm \todo{citation}, which allows users to coordinate a set of nodes running docker (denoted a swarm). Nodes in a swarm, among many other features, may perform Multi-host networking, which consists in integrating containers from different nodes into a unified network, where they are automatically assigned IP addresses and can communicate, regardless of the machine each docker is executing on. To setup the experimental scenario, we developed a set of scripts in both BASH, Python and GO to create, orchestrate, and decomission (when needed) containers such that all containers are inserted into a unified network(and assigned IPS in a pre-determined range), which are then loaded with all the necessary executables to run the experiments.

\subsection{Node capacity and connection delays}

As previously mentioned, in order to emulate a real-world scenario, where nodes have limited capacity and their connections have delays, there was the need to apply these limitation in a realistic way. To do so, we used data from real-world readings of real-world scenarios obtained from WonderNetwork \todo{https://wondernetwork.com/ meter como cite}, which is a network of 252 nodes, spread across 88 countries in 6 continents. This network provides, in addition to node metadata (city, country, among others), a set of latency measurements to every other in the network (including themselves). We extracted this information, and there as there was the need to test the framework with larger node counts (up to 750 nodes), the data points were multiplied by 5 times. 

Then, as the obtained data from this network did not contain bandwidth information for each node, we used the country, provided by WonderNetwork, to assign bandwidth values according to the list of bandwidth per country provided by speedtest.net \cite{speedtest_global_index}. Provided the purpose of this framework is to perform on cloud-edge scenarios, composed by nodes inside and outside of the data-center (DC), where nodes outside the DC have lower networking capacity comparatively to nodes running inside the DC, we divided each data point by 12x (representing the nodes running outside of the DC), and divided the first N nodes by 2.5x, (corresponding to the number of nodes representing the data centers).

Provided with the networking capacity and the latency matrix for all connection pairs, there was the need to both limit the networking capacity and inject latency in the containers executing the protocols for the experiments. To achieve this, we used a tool called Traffic Control (TC) \todo{cite}, this tool is a traffic shaping tool that performs shaping, scheduling, policing and dropping of network packets through the configuration of the kernel packet scheduler. This tool sets up sets of queuing systems and mechanisms by which packets are received and transmitted, then queue (also denominated qdisc) or class specific policies decide which (and whether) packets to accept at what rate on the input of an interface and determining which packets to transmit in what order at what rate on the output of an interface. 

In our case, we used this tool to both limit the available inbound/outbound bandwidth on the container interfaces and to inject delays in all connection pairs. In order to limit the available bandwidth, we used hierarchical token buckets (htb), which are classful queueing disciplines that employ a complex token-borrowing system to ensure shaping of traffic according to a certain configurable rate. HTB requires programmers to set up a hierarchical class structure, where child classes, attached to a qdisc, maniulate packet order and apply certain policies according to configuration.

For our benchmark, we made use of rate-limiting policies, which employ a token borrowing mechanism that functions in the following manner: whenever a certain child class reaches the maximum of its rate, it borrows tokens (up to its \textbf{ceiling} value) from the parent class (if there is a parent, and the parent has available tokens). If the parent class is also limited, then the sum of its child classes will be limited to its rate. In our experimental configuration, each container creates two default queues (or qdiscs) attached to the inbound and outbound networking interfaces. Then, two HTBs are attached to the default inbound and outbound qdiscs (with the respective inbound and outbound bandwidth rate). After this, for both the outbound and inbound classes, two child classes are installed: one intended for latency measurements and keepalive traffic (specific UDP traffic on a pre-configured port); and the other for the remaining traffic. The inbound and outbound classes responsible for measurement traffic are assigned a fixed rate of 500kb, and the inbound and outbound default traffic classes are assigned a rate corresponding to the configured download and outbound rate for the container minus the 500kb rate for the measurements class. Then, for all the outbound classes (measurement and default traffic), we set up another set of HTB classes for each other container with a very low rate of 6kb and ceiling rate corresponding to the parents' class. This setup forces child classes to borrow tokens from the parent class, and be limited by the intended bandwidthrate.

For each of these leaf classes, we attached a netem qdisc which applies a delay to each packet corresponding to the latency between the origin and the target container. In our experimental setup, packets are forwarded using filters, in the case of the measurement traffic the filtering was performed via installing a high-priority filter verifying the source and destination ports of the packets and sending it to the measurement classes. The remaining traffic was then forwarded to the default traffic class. The Routing from these two outbound classes to the leaf classes is done via filtering the destination IP address of the packets.  The objective of separating the traffic in these two classes is to prevent cases where the applicational traffic is high (i.e. testing information dissemination) and the delay caused by the high usage of the data channels would interfere with the measurement packets, leading to incorrect latency measurements and consequent instability during experiments for both deMMon and the baseline overlay protocols. 

Experiments presented in this work were carried out using the Grid'5000 testbed, supported by a scientific interest group hosted by Inria and including CNRS, RENATER and several Universities as well as other organizations (see https://www.grid5000.fr) \todo{cite}. The hardware from this testbed used to carry the experiments was sets of 10 physical nodes for experiments with 50 and 250 logical nodes, and sets of 30 physycal nodes for experiments with both 500 and 750 logical nodes. Each physycal machine is equipped with 2 x Intel Xeon E5-2630 v3 and 128 GiB of RAM, and is executing Linux Debian version 4.19.104-2 and Docker version 20.10.7. The results were obtained through logging the relevant results to disk, and then processing the obtained logs to extract the intended information posterior to the end of the experiments.

Provided with the experimental setup, we now explain the steps taken and the results obtained in the overlay protocols' evaluation.

\section{Overlay Protocol: Experimental Evaluation} \label{sec:overlay_proto_eval} \input{Chapters/evaluation/memb_proto_evaluation.tex}

\section{Aggregation Protocol: Experimental Evaluation} \label{sec:agg_proto_eval} \input{Chapters/evaluation/agg_proto_evaluation.tex}

\section{Summary}

In this chapter, we studied through experimentation the applicability of our devised decentralized aggregation and information dissemination framework, named DeMMon. We began by providing the system model in which we tested our solution (section \ref{sec:exp_setting_conf}), which aims to emulate a realistic cloud-edge scenario, composed by nodes with heterogenous networking capacity, and distributed along multiple places of the globe. Following, in section \ref{\label{sec:overlay_proto_eval}, we provide the obtained results from the experimental evaluation of the developed membership protocol. In this section, we tested the applicability of this protocol in its capacity to build a latency-optimized network by comparing it with implementations of state-of-the-art baselines in realistic scenarios with multiple node counts and failures. We showed that the implemented protocol is fault-tolerant (within its system model), and although its latency total is not the lowest, if only accounting for its most heavily used connections (the parent connections in the tree), then its total cost is the lowest. While still in this section, we evaluated the information dissemination capabilities of the devised protocol against the same baseline protocols, paired with two dissemination protocols: a simple flood protocol and PlumTree. Overall, we showed that our protocol is a competitive alternative in this regard, as in our tests it performed better than PlumTree when paired with any baseline membership protocols in all conditions, and performed better in lower node counts across all conducted scenarios.

Finally we concluded the chapter with section \ref{sec:agg_proto_eval}, where we validated the implemented monitoring primitives and compared the obtained results against multiple configurations of a popular baseline solution: Prometheus. We showed that the devised decentralized monitoring solution obtains results comparable to those obtained by Prometheus, and that it provides a higher degree of fault-tolerance, as it does not require manual configuration to recover from failures. 
\chapter{Related Work}
\label{cha:related_work}

\section{Decentralizing clouds}

\subsubsection{Fog Computing}

\subsubsection{Edge Computing}

\subsubsection{Osmotic Computing}

\section{Peer to Peer} % (fold)
\label{sec:p2p}

The Peer to Peer (P2P) paradigm has been extensibly used to implement distributed, scalable, fault-tolerant services, it overcomes limitations such as scalability and fault-tolerance that arise from the client-server model. 

Collaboration is the foundation of P2P systems, it is the foundation of the scalability provided by P2P. participants of the system (peers) perform tasks that contribute towards a common goal which is beneficial for the overall functioning of the system. Collaboration enables P2P systems that accomplish tasks would otherwise be impossible by an individual server.  % citations

Peers contribute with a portion of their resources (such as computing, memory or network bandwidth, among others) to other participants as well as consume resources from other peers in the system. This contrasts with the traditional client-server paradigm where the consumption and supply of resources is decoupled. Finally, fault tolerance is achieved by the absence of a centralized point of failure.

There are many types of services that are based on P2P systems, however, most popular types of services build on this paradigm are: resource location, content delivery, cryptocurrency or blockchain, etc. Popular services built on this paradigm are file sharing applications, (e.g. Napster, BitTorrent, Emule and Gnutella), cryptocurrencies (Bitcoin), streaming (Skype), anonymity (TOR) among others. 

One factor that imposes a crucial difference in how these systems behave is the membership information, a participant in the system that knows every other participant in the system is said to have full membership information, the alternative is called partial membership, where a peer is only aware of a partial number of elements in the system.

Full membership systems are usually employed in small to medium sized storage solutions like on One-Hop DHTs e.g. Kademlia \cite{10.1007/3-540-45748-8_5} and Amazon's Dynamo \cite{decandia2007dynamo}. However, full membership solutions tend to have scalability problems and behave poorly in the face of churn (participants leaving and entering the system), which increases the workload necessary to maintain the membership information up-to-date. 

% citations

Partial membership systems rely on some membership mechanism that restricts each peer to only have a few neighboring relations that are used to perform communication (usually through message exchanges) between each other. Similarly to full membership, the number of connections a peer has  dictates the scalability of the system, where systems with larger views will have a harder time maintaining them. The accumulation of the partial views of all peers in the system dictates the topology of the overlay network.

\section{Topology Management}
\label{sec:topology_mgmt}

Topology management consists in the creation and management of an \textbf{overlay network}, which consists in a logical network built on top of another network (usually the internet). Elements of overlays are connected through virtual links that are a combination of one or more underlying physical links.

The way the aforementioned links are organized dictate the type of overlay: if the links are logically organized by some metric, we call it a \textbf{structured overlay}. On contrary, when there are no restrictions form the links, we call it an \textbf{unstructured overlay}. Services then leverage on the topologies that are tailored as closely as possible towards application requirements to build services.

\subsection{Structured overlays}

Structured overlays impose restrictions over the neighboring relations that can be established among peers, often based on the identifier of each participant of the system. Usually these identifiers are unique and are independently generated by each peer, then peers employ a global coordination mechanism to tightly control the topology. Examples of common resulting topologies of structured overlays are rings, meshes, hypercubes, among others.

\subsubsection{Distributed Hash tables}

As previously mentioned, a widely used type of structured overlays are distributed hash tables (DHT
). Often, DHT's provide application-level routing by employing topologies that can be transversed in logarithmic time. Then, using hash functions to map objects (files, multimedia, messages, etc.) to the peer identifier space, and assigning a key-space interval to each peer, peers can find the peer responsible for any key in a logarithmic number of steps. 

DHT's have been extensibly used to support many large-scale different types of services (publish-subscribe, resource location, monitoring, among others) and are especially used in Cloud-based environments. Their popularity derives from providing the previously mentioned functionality while maintaining very little membership information (typically 1\% of the peers).

However, a DHT's behavior depends on the correctness of the topology, and when in the presence of high churn or network partitions / failures, the correctness of the topology is harder to maintain, nodes must exchange more messages to ensure that the topology is correct. Additionally, whenever a node fails, DHT's rely on the correctness of the routing infrastructure to replace the failed node,
this means that if there is a failure in the routing infrastructure, the DHT may become unrepairable.

% citations

Following we present some popular implementations of relevant DHT's:

\paragraph{ \textbf{Chord} \cite{stoica2003chord} is a distributed lookup protocol that addresses the need to locate the node that stores a particular data item, it specifies how to find the locations of keys, how nodes recover from failures, and how nodes join the system. 

Chord assigns each node and key an m-bit identifier that is uniformly distributed in the id space (peers receive roughly the same number of keys). Peers are ordered by identifier in a clockwise circle, then, any key \(k\) is assigned to the first peer whose identifier is equal or follows k in the identifier space. 

Additionally, Chord implements a system of "shortcuts"\ called the \textbf{finger table}. The finger table contains at most \(m\) entries, each $ith$ entry of this table corresponds to the first peer that succeeds a certain peer \(n\) by \(2^{ith}\) in the circle.
This means that whenever the finger table is up-to-date, lookups only take logarithmic time to finish.

Chord has been tested in many environments...

} 
    
\paragraph{ \textbf{Pastry} \cite{rowstron2001pastry} is a DHT that assigns a 128-bit node identifier (nodeId) to each peer in the system. The nodes are randomly generated thus uniformly distributed in the 128-bit nodeId space. Nodes store values whose keys are also distributed in the nodeId space.

key-value pairs are stored among nodes that are numerically closest to the key. This is accomplished by: in each routing step, messages are forwarded to nodes whose nodeId shares a prefix that is at least one bit closer to the key. If there are no nodes available, Pastry routes messages towards the numerically closest nodeId.

This routing technique accomplishes routing in O(log N), where N is the number of Pastry nodes in the system. This protocol has been widely used and tested in applications such as Scribe \cite{10.1007/3-540-45546-9_3} and PAST \cite{990064}. Limitations from using Pstry arise from the use of a numeric distance function towards the end of the routing process, which creates discontinuities at some node ID values, and complicates attempts at formal analysis of worst case behavior.

}

\paragraph{ \textbf{Kademlia} \cite{10.1007/3-540-45748-8_5} is a DHT with provable consistency and performance in a fault-prone environment. Kademlia nodes are assigned 160-bit identifiers uniformly distributed in the ID space. 

Peers route queries and locate nodes by employing a XOR-based distance function that is symmetric and unidirectional. Each node in Kademlia is a router whose routing tables consist of shortcuts to peers whose \textbf{xor distance} is between \(2^{i}\) by \(2^{i + 1}\) in the ID space. Intuitively, and similar to Pastry, "closer" nodes are those that share a longer common prefix.

The main benefits that Kademlia draws from this approach are: nodes learn routing information from receiving messages, there is a single routing algorithm for the whole routing process (unlike Pastry) which eases formal analysis of worst-case behavior. Finally, Kademlia exploits the fact that node failures are inversely related to uptime by prioritizing nodes that are already present in the routing table.
    
}

\paragraph{ \textbf{Kelips} \cite{gupta2003kelips} exploits increased memory usage and constant backgroung communication to achieve O(1) lookup time and message complexity. 

Kelips nodes are split in $k$ affinity groups split in the intervals [0,$k-1$] of the ID space, thus, with $n$ nodes in the system, each affinity group contains frac{n}{k} peers. Each node stores a partial set of nodes contained in the same affinity group and a small set of nodes lying in foreign affinity groups.

Through increased communication cost by employing Gossip protocols and memory consumption (O(\sqrt{n}) assuming a proportional number of files and peers in the system and a fixed view of nodes in foreign affinity groups), Kelips achieves O(1) time and message complexity in lookups. However, system scalability is limited when compared to Pastry, Chord or Kademlia. }

\paragraph{ \textbf{Tapestry} \cite{tapestry} Is a DHT similar to pastry where messages are incrementally forwarded to the destination digit by digit (e.g. ***8 -> **98 -> *598 -> 4598). Lookups have logb(n) time complexity where b is the base of the ID space. 

A system with n nodes has a resulting topology composed of n spanning trees, where each node is the root of its own tree. Because nodes assume that the preceding digits all match the current node's suffix, ir only needs to keep a constant size of entries at each route level. Thus, nodes contain entries for a fixed-sized neighbor map of size $b.\logb(N)$.

% TODO falar de surrogate routing
\textcolor{red}{surrogate routing}}

% TODO falar de otimizacoes fixes observadas:
% lazyness a montar a rede (usar mensagens de servicos)
% manter peers antigos (churn inversamente proporcional a uptime)
% formar grupos para reduzir routing (increased background communication)
% ao usar prefix routing consegue-se logb(n) routing
% Xor-distance vs numeric distance



\subsection{Random overlays}

\subsection{Self-adapting overlays}

\section{Aggregation} 
\label{sec:aggregation}

\subsection{Types of aggregation}

\subsection{Relevant aggregation protocols}

\section{Resource Discovery} 
\label{sec:res_discvovery}

\section{Offloading computation to the edge} 
\label{sec:offloading_comp}
\subsection{Device monitoring}

In order to adapt edge computing applications to changes in the environment and ensure that the above requirements can be met, it is necessary to tailor the monitoring system to support the whole spectrum of underlying infrastructures (section \ref{sec:offloading_computation}). 

A modern approach towards deploying systems is to deploy them in loosely coupled independent components running some form of virtualization software. Virtualization is the building block of resource management systems, as it enables co-deployment of logical machines in the same physical node.

\subsubsection{VM monitoring ?}

In virtual machines (VMs), all the physical resources can be virtualized (CPU, memory, disk and network). Multiple VMs can co-deployed in the same physical node and thus share resources among each other. 

In order to have efficient resource utilization and prevent any problems in the virtualized resources, monitoring of VMs is critical. This can be best achieved by tracking the utilization of the virtualized resources, mainly usage of CPU, memory, storage and network.

\begin{itemize}

    \item \textbf{CPU} usage tracks the amount of usage of the CPU as a percentage of all available CPU to the machine. It is never desirable that the CPU usage reaches 100\%, as queues start filling up and it means that the device has run out of capacity for processing tasks.
    
    \item \textbf{Memory} indicates the amount of virtualized memory left.
    
    \item \textbf{Disk usage} tracks the amount of data read or written by a VM. Alternatively, it can indicate the percentage of used space.
    \item \textbf{Network usage} consists in the volume of traffic on a specific network interface of the VM, either external or internal traffic.

\end{itemize}

Although VMs are widely present in the cloud infrastructure, we believe their applicability towards edge scenarios is limited, due to the fact that they have significant start up time (having to start-up an OS).

\subsubsection{Container monitoring}

Containers are the recent alternative to VMs, as previously mentioned, containers do not require an OS to boot up \cite{}, and the images that compose them are significantly smaller. Container-based virtualization can be compared to an OS running on bare-metal in terms of memory, CPU and disk usage, however at a cost of network utilization \cite{preeth2015evaluation}, which makes them an attractive options towards resource-constrained devices.

Due to their lightweight nature, it is possible to deploy container-based applications (e.g. microservices), which can perform fast migration across nodes in the edge environment in order to improve QoS. This flexibility towards the migration process is an efficient tool to deal with many challenges such as load balancing, scaling, resource reallocation and fault-tolerance. 

There are many container-specific tools which provide statistics about a given container, most use REST APIs to expose this functionality to external entities. Docker, which is a container provider, \textcolor{red}{citação?} has a built tool called \textit{docker stats} which provides runtime metrics for a given container.

Prometheus is is an open source system for monitoring and alerting, originally built at SoundCloud. Prometheus is a tool which scrapes data from containers using HTTP and stores them into time series, then, it offers a platform towards configuring alerts, which are triggered whenever a certain criteria is met. However, the data collection is centralized, and thus its scalability is limited. 

\textcolor{red}{// TODO falar de quais metricas (como nas vms) ? }

\subsection{End-to-end link monitoring}

Given that the edge infrastructure envisions cooperation from all devices in the path from the origin of the data to the DC, devices need to be interconnected across an underlying infrastructure which is continuously changing. This raises concerns about the network quality of links between devices across the system, especially if they are running time-critical services. 

It is paramount to analyze how to monitor and improve link quality, for providing traffic locality, latency, among others. According to the literature \cite{}, the most popular metrics to analyze are:

\begin{enumerate}

    \item Network throughput, which is the average rate of successful data transfer through a network connection.
    
    \item Latency, which consists in how long a packet takes to travel across a link from one endpoint to another
    
    \item Packet loss, which consists in how many packets are lost when traveling towards their destination.
    
    \item Jitter is the variation in latency of sequential received packets. 
\end{enumerate}

\subsection{Aggregation}

\textbf{Aggregation} is an essential building block towards monitoring distributed systems, it enables the determination of important system wide properties in a decentralized manner \cite{DBLP:journals/corr/abs-1110-0725}. 

Aggregation consists in computing an aggregation function over a set of input values where each node has one input value. Common aggregation functions consist in sum, count, average, min, max.

Towards monitoring edge devices and tasks running on them, aggregation is paramount, examples of usages are (e.g. computing the average latency of the closest available  service that meets a certain criteria; counting nearby available computing resources that can be used to offload services, or identify hotspots by aggregating the average system load in certain areas).

Given this, it is important to understand the taxonomy of aggregation functions. There are two properties of aggregation functions: \textit{decomposability} and \textit{duplicate sensitiveness}.

\textbf{Decomposability}

For some aggregation functions, we may need to involve all elements in the multiset, however, for memory and bandwidth issues, it is impractical to perform a centralized computation, hence, the aim is to employ \textit{in-transit computation}. In order to enable this, it is required that the aggregation function is \textbf{decomposable}. 

Intuitively, a decomposable aggregation function is one where a function may be composed defined as a composition of other functions. Decomposable functions may \textit{self-decomposable}, which intuitively means that the aggregated value is the same for all possible combinations of all sub-multisets partitioned in the multiset. This happens whenever the applied function is commutative and associative (e.g. min, max, sum, count).

A canonical example of a decomposable function that is not self-decomposable is average, which consists in the sum of all pairs divided by the count of peers that contributed to the aggregation.

The second property of aggregation is \textbf{duplicate sensitiveness}, and it is related to wether a given value occurs several times in a multiset. Depending on the aggregation function used, the presence of repeated values may influence the result, it is said that a function is \textbf{duplicate sensitive} if the result of the aggregation function is influenced by the repeated values (e.g. SUM). Conversely, if the aggregation function is \textbf{duplicate insensitive}, it can be successfully repeated any number of times to the same multiset without affecting the result (e.g. MIN and MAX).

Table \ref{table:aggregation_functions} classifies popular aggregation functions in function of decomposability and duplicate sensitiveness as found in \cite{DBLP:journals/corr/abs-1110-0725}:

\begin{table}[]
    \begin{tabular}{|l|l|l|l|}
    \hline
                          & \multicolumn{2}{l|}{Decomposable} & Non-Decomposable  \\ \hline
                          & Self-decomposable    &                             &  \\ \hline
    Duplicate insensitive & Min, Max             & Range     & Distinct Count    \\ \hline
    Duplicate sensitive   & Sum, Count           & Average   & Median, Mode     \\ \hline
    \end{tabular}
    \caption{popular aggregation functions in function of decomposability and duplicate sensitiveness}
    \label{table:aggregation_functions}
\end{table}

Building on the concepts of duplicate sensitiveness and decomposability, we show that aggregation functions present their own particularities which dictate their applicability in particular scenarios. For example, a Min or Max function may be easier to implement with a simpler algorithm, while Sum, Count and Average require extra considerations. This presents a limitation towards calculating exact aggregations in large scale systems, to circumvent this, some systems do not require obtaining exact aggregated values to perform near optimally  (e.g. estimating the system size in order to select the optimal fanout for a gossip system only requires an estimation of the magnitude of the system). 

\subsubsection{Aggregation techniques}

Following, we present a taxonomy of aggregation techniques for edge devices, for each technique, we provide context and discuss its possible advantages and limitations in the edge environment.

\subsubsection{Hierarchical}

\textbf{Tree-based} approaches leverage directly on the decomposability of aggregation functions. Aggregations from this class depend on the existence of a hierarchical communication structure, (e.g. a spanning tree) with one root (sink node). Aggregations take place by splitting inputs into groups and aggregating values bottom-up in the hierarchy. 

Commonly, hierarchical aggregation systems have nodes whose roles are \textit{aggregators} or \textit{forwarders}, intuitively, aggregators compute the aggregation functions and forward results to forwarders who then retransmit the results to upper levels in the hierarchy. In the absence of faults, the correct final result is obtained in the sink node.

\textbf{Cluster-based} techniques rely on clustering the nodes in the network according to a certain criterion (e.g. latency, energy efficiency). In each cluster a representative is responsible for local aggregation and for transmitting the results to other nodes. 

Hierarchical approaches, due to taking advantage of device heterogeneity, are attractive in edge environments. However, due to the low computational power of devices, not all nodes may be able to handle the additional overhead of maintaining the hierarchical topology.

\subsubsection{Averaging}

Averaging aggregation consists in the continuous computation and exchanging of partial averages data among all active nodes in the aggregation process. In this type of systems, after a few rounds, all nodes usually converge to the correct value with high accuracy, as shown in \cite{gossip_aggregation}.

This type of aggregation is attractive for gossip protocols, where nodes may employ varied gossip techniques to continuously share and update their values with random neighbors.

Algorithms from this category are also attractive to use in edge environments, because they are accurate while employing random unstructured overlays, which retain their fault-tolerance and resilience to churn.

\textbf{Sketches} are fixed-size data structures that hold a \textit{sketch} of all network values. Multiple sketches are usually forwarded throughout the system, and nodes who forward sketches apply (usually commutative and associative) operations to update and merge them. \textcolor{red}{functioning and edge discussion}

% completar funcionamento

\textbf{Digests} are an aggregation technique that gathers a representation of all system values, it supports complex aggregation functions such as Median and Mode. In short, algorithms employ a fixed-size data structures commonly composed of a set of values and associated counters) which compacts the data distribution (e.g. into a histogram). \textcolor{red}{edge discussion}

\textbf{Counting} algorithms target the same aggregation function: Count, algorithms from this class usually employ some randomized procedure to achieve a probabilistic approximation of the population size.

\subsection{Relevant aggregation protocols}

In this subsection we will analyze relevant aggregation protocols that ilustrate some techniques discussed above.

\subsubsection{TAG: Tiny AGgregation}

\textbf{TAG: Tiny AGgregation}\cite{Madden2002} is a service for aggregation in low-power, distributed, wireless sensor networks. TAG distributes queries in the network in a time and power-efficient manner by employing a hierarchical aggregation pattern. For each aggregation procedure, there is a \textit{root} nodes which broadcasts a message to start the tree-building process, each message contains two fields: a level and a an ID. Whenever a node without an assigned level receives a tree-building message, it assigns its own level as the message level plus one, and its own parent as the message sender. Then, it reassigns the level and ID to its own and forwards the message to other nodes. Then, whenever a node wishes to send a message to the root, it simply forwards the message bottom-up in the tree. The formed topology allows the computation of Count, Maximum, Minimum, Sum and Average. It is important to notice that the formed tree will be unbalanced as a function of the underlay latency and processing time.

\subsubsection{DECA}

\textbf{DECA} \cite{Artigas2006} \textcolor{red}{//TODO}

\subsubsection{Astrolabe}

\textbf{Astrolabe} \cite{Renesse2003} \textcolor{red}{//TODO}

\subsubsection{SingleTree \& MultipleTree}

\textbf{SingleTree \cite{} and MultipleTree \cite{}} \textcolor{red}{//TODO}


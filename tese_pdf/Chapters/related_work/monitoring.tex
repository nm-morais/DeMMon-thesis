
In this section we will cover \textbf{resource monitoring}, which is paramount for making effective management decisions regarding task allocations and managing the overlay network. Resource monitoring consists in tracking the state of a certain aspect of a system, such as the device status, the capacity of links between devices, the status of available resources in a given zone of the system, among others. 

\subsection{Device Monitoring}

A particularly hard problem in resource monitoring is fault detection, given the need to ensure each component is monitored by at least one non-faulty component, even in the face of joins, leaves and failures of both nodes and network infrastructure. Most fault-detectors rely on heartbeats, which consist in a peer sending a message periodically to another peer in order to signal that it is functioning correctly.

\textcite{leitao2008large} proposes a decentralized device monitoring system by employing Hyparview \cite{Hyparview} as a decentralized monitoring fault detector, given the fixed number of active connections, which ensures overlay connectivity, each peer ensures has at least another non-faulty component monitoring it through the active TCP connection. In addition to tracking device health, it is paramount to collect metrics regarding the operation of the device, such as:

\begin{enumerate}

    \item Network related information: devices need to be interconnected across an underlying infrastructure which is continuously changing. This raises concerns about the network quality of links between devices across the system, especially if they are running time-critical services. Given this, it is paramount to track network related information such as bandwidth, latency and link status.
    
    \item Memory related information: either related to volatile memory or persistent memory, it is important to track how much of it is currently free and how much memory is actively being used.
    
    \item CPU information: the utilization of the CPU (user, sys, idle, wait).
    
\end{enumerate}

\subsection{Container Monitoring}

It is paramount to obtain insight about the tasks that are running atop the resource sharing platform. As previously mentioned, containers are the solution which incurs less overhead when it comes to sharing resources in the same node, given this, we now study tools which monitor the status of containers and the applications executing inside them. 

\textit{Docker} \cite{docker} has a built tool called \textit{docker stats} \cite{docker_stats} which provides a live data stream of metrics related to running containers. It provides information about the network I/O, cpu and memory usage, among others. 

\textit{Container Advisor} \cite{cAdvisor} (cAdvisor) is a service which analyzes and exposes resource usage and also performance data from running containers. The information it collects consists of resource isolation parameters, historical resource usage and network statistics. cAdvisor includes native support for Docker containers and supports a wide variety of other container implementations.

\textit{Agentless System Crawler}  (ASC) \cite{cloudviz_2019} is a monitoring tool with support for containers. It collects monitoring information from running containers including metrics, system state and configuration information. It provides the ability to build two types of plugins: function  plugins for on-the-fly data aggregation or analysis, and output plugins for target monitoring and analytics endpoints.

There are many other tools which offer the ability to continuously collect metrics about running containers, however, if we were to continuously store and transmit these metrics, the amount of communication and processing needed to do this would quickly overload the system. Consequently, there is the need to reduce the size of the data through a process called \textit{aggregation}.

\subsection{Aggregation}

Aggregation consists in the determination of important system wide properties in a decentralized manner \cite{DBLP:journals/corr/abs-1110-0725}, it is an essential building block towards monitoring distributed systems. For example, towards computing the average of available computing resources in a certain part of the network, or towards identifying application hotspots by aggregating the average resource usage in certain areas, among many other uses. There are two properties of aggregation functions: \textit{decomposability} and \textit{duplicate sensitiveness}.

\subsubsection*{Decomposability}

For some aggregation functions, we may need to involve all elements in the multiset, however, for memory and bandwidth issues, it is impractical to perform a centralized computation, hence, the aim is to employ \textit{in-transit computation}. In order to enable this, it is required that the aggregation function is \textbf{decomposable}. 

Intuitively, a decomposable aggregation function is one where a function may be composed defined as a composition of other functions. Decomposable functions may \textit{self-decomposable}, which intuitively means that the aggregated value is the same for all possible combinations of all sub-multisets partitioned in the multiset. This happens whenever the applied function is commutative and associative (e.g. min, max, sum, count). A canonical example of a decomposable function that is not self-decomposable is average, which consists in the sum of all pairs divided by the count of peers that contributed to the aggregation.

\subsubsection*{Duplicate sensitiveness}

The second property of aggregation is \textbf{duplicate sensitiveness}, and it is related to wether a given value occurs several times in a multiset. Depending on the aggregation function used, the presence of repeated values may influence the result, it is said that a function is \textbf{duplicate sensitive} if the result of the aggregation function is influenced by the repeated values (e.g. SUM). Conversely, if the aggregation function is \textbf{duplicate insensitive}, it can be successfully repeated any number of times to the same multiset without affecting the result (e.g. MIN and MAX). Table \ref{table:aggregation_functions} classifies popular aggregation functions in function of decomposability and duplicate sensitiveness as found in \cite{DBLP:journals/corr/abs-1110-0725}.

\begin{table}[]
    \begin{tabular}{|l|l|l|l|}
    \hline
                          & \multicolumn{2}{l|}{Decomposable} & Non-Decomposable  \\ \hline
                          & Self-decomposable    &                             &  \\ \hline
    Duplicate insensitive & Min, Max             & Range     & Distinct Count    \\ \hline
    Duplicate sensitive   & Sum, Count           & Average   & Median, Mode     \\ \hline
    \end{tabular}
    \caption{popular aggregation functions in function of decomposability and duplicate sensitiveness}
    \label{table:aggregation_functions}
\end{table}

Building on the concepts of duplicate sensitiveness and decomposability, we show that aggregation functions present their own particularities which dictate their applicability in particular scenarios. For example, a Min or Max function may be easier to implement with a simpler algorithm, while Sum, Count and Average require extra considerations. 

This presents a limitation towards calculating exact aggregations in large scale systems, to circumvent this, some systems do not require obtaining exact aggregated values to perform near optimally (e.g. estimating the system size in order to select the optimal fanout for a gossip system only requires an estimation of the magnitude of the system). 

\subsection{Aggregation techniques}

Following, we provide context about the taxonomy of aggregation techniques:

\subsubsection*{Hierarchical aggregation}

\textbf{Tree-based} approaches leverage directly on the decomposability of aggregation functions. Aggregations from this class depend on the existence of a hierarchical communication structure, (e.g. a spanning tree) with one root (sink node). Aggregations take place by splitting inputs into groups and aggregating values bottom-up in the hierarchy. 

%Commonly, tree-based systems have nodes whose roles are \textit{aggregators} or \textit{forwarders}, intuitively, aggregators compute the aggregation functions and forward results to forwarders who then retransmit the results to upper levels in the hierarchy. In the absence of faults, the correct final result is obtained in the sink node.

\textbf{Cluster-based} techniques rely on clustering the nodes in the network according to a certain criterion (e.g. latency, energy efficiency). In each cluster a representative is responsible for local aggregation and for transmitting the results to other nodes. 

Hierarchical approaches, due to taking advantage of device heterogeneity, are attractive in edge environments. However, due to the low computational power of devices, not all nodes may be able to handle the additional overhead of maintaining the hierarchical topology.

\subsubsection*{Continuous aggregation}

Continuous aggregation consists in the continuous computation and exchange of partial averages data among all active nodes in the aggregation process. This type of aggregation is attractive for gossip protocols, where nodes may employ varied gossip techniques to continuously share and update their values with random neighbors. Algorithms from this category are also attractive to use in edge environments, because they provide high accuracy while employing random unstructured overlays \cite{gossip_aggregation}, consequently, the aggregation process retains the fault-tolerance and resilience to churn from these overlays.

\textbf{Sketches} are fixed-size data structures that hold a \textit{sketch} of all network values. Multiple sketches are usually forwarded throughout the system, and nodes who forward sketches apply (usually commutative and associative) operations to update and merge them.

\textbf{Digests} are an aggregation technique which gathers a representation of all system values, it supports complex aggregation functions such as Median and Mode. In short, algorithms employ a fixed-size data structures commonly composed of a set of values and associated counters) which compacts the data distribution (e.g. into a histogram).

%\textbf{Counting} algorithms target the same aggregation function: Count, algorithms from this class usually employ some randomized procedure to achieve a probabilistic approximation of the population size.

%\subsection{Relevant aggregation protocols}

%In this subsection we will analyze relevant aggregation protocols that illustrate some techniques discussed above.

%\subsubsection{TAG: Tiny AGgregation}

%\textbf{TAG: Tiny AGgregation}\cite{Madden2002} is a service for aggregation in low-power, distributed, wireless sensor networks. TAG distributes queries in the network in a time and power-efficient manner by employing a hierarchical aggregation pattern. For each aggregation procedure, there is a \textit{root} nodes which broadcasts a message to start the tree-building process, each message contains two fields: a level and a an ID. Whenever a node without an assigned level receives a tree-building message, it assigns its own level as the message level plus one, and its own parent as the message sender. Then, it reassigns the level and ID to its own and forwards the message to other nodes. Then, whenever a node wishes to send a message to the root, it simply forwards the message bottom-up in the tree. The formed topology allows the computation of Count, Maximum, Minimum, Sum and Average. It is important to notice that the formed tree will be unbalanced as a function of the underlay latency and processing time.

%\subsubsection{SingleTree} 

%\textbf{SingleTree} \cite{} \textcolor{red}{//TODO}

%\subsubsection{MultipleTree} 

%\textbf{MultipleTree} \cite{} \textcolor{red}{//TODO}

%\subsubsection{DECA} \textbf{DECA} \cite{Artigas2006} \textcolor{red}{//TODO}

\subsection{Monitoring systems}

Following, we study popular monitoring systems in the literature, for each system we analyze its advantages and drawbacks, followed by a discussion with the systems' applicability in an edge environment.

\subsubsection{Astrolabe}

\textbf{Astrolabe} \cite{Renesse2003} is a distributed information management platform which aims at monitoring the dynamically changing state of a collection of distributed resources. 

Astrolabe introduces a hierarchical architecture defined by zones, where a zone is recursively defined to be either a host or a set of non-overlapping zones.Each zone (minus the root zone) has a local identifier, which is unique within the zone where it is contained. Zones are globally identified by their \textit{zone name}, which consists of the concatenation of all zone identifiers within the path from the root to the zone.

Associated with each zone there is a Management Information Base (MIB), which consists in a set of attributes from that zone. Zone attributes are not directly writable, instead they are generated by aggregation functions contained in special entries in the MIB. Leaf zones are the exception to the aforementioned mechanism, leaf zones contain \textit{virtual child zones} which are directly writable by devices within that virtual child zone.

The aggregation functions which produce the MIBs are contained in \textit{aggregation function certificates} (AFCs), which contain a user-programmable SQL function, a timestamp and a digital signature. In addition to the function code, AFCs may contain other information, an \textit{Information Request AFC},  specifies which information to retrieve from each participating host, and how to summarize the retrieved information. Alternatively, we may have a \textit{Configuration AFC} which specifies runtime parameters that applications may use for dynamic configuration.

Astrolabe employs gossip, which provides an eventual consistency model: if updates cease to exist for a long enough time, all the elements of the system  converge towards the same state. This is achieved by employing a gossip algorithm which selects another agent at random and exchanges zone state with it. If the agents are within the same zone, they simply exchange information relative to their zone. Conversely, if agents are in different zones, they exchange information relative to the zone which is their least common ancestor.

Not all nodes gossip information, within each zone, a node is elected (the authors do not specify how) to perform gossip on behalf of that zone, additionally, nodes can represent nodes from other zones, in this case, nodes run one instance of the gossip protocol per represented zone. The number of represented zones is bounded by the number of levels in the Astrolabe tree.

Astrolabe relies on clock time from the devices enforce an order among MIBs. Originally, it compared timestamps contained in MIBs to impose an order among them, however, as the system scaled, this approach is became unreliable. To circumvent this, Astrolabe agents store the last MIB from each agent that gossiped with them and only compare the timestamps of MIBs which originated from the same representative. Given this, attribute updates may not be monotonic, due to the fact that aggregations do not take into account the time where the leaf attribute was updated. 

An agents' zone is defined by the system administrator, which is a potential limitation towards scalability, given that configuration errors have the potential to heavily reduce system latency and traffic locality. Additionally, the original authors state that the size of gossip messages scales with the branching factor, often exceeding the maximum size of a UDP packet. Other limitations which arise from using Astrolabe are the high memory requirements per participant due to the high degree of replication and zone representatives are a point of failure in each zone. 

\subsubsection{Ganglia}

\textbf{Ganglia} \cite{massie2004ganglia} is a scalable distributed monitoring system for high performance computing systems, namely clusters and grids. In short, Ganglia groups nodes in clusters, in each cluster, there are representative cluster nodes which federate devices and aggregate internal cluster state. Then, representatives aggregate information in a tree of point-to-point connections.

Ganglia relies on IP multicast to perform intra-cluster aggregation, it is mainly designed to monitor infrastructure monitoring data about machines in a high-performance computing cluster. Given this, its applicability is limited towards edge environments: (1) clusters are situated in stable environments, which contrasts with the edge environment; (2) it relies on IP multicast, which has been proven not to hold in a number of cases; (3) has no mechanism to prevent network congestion; (4) the project info page only claims scalability up to 2000 nodes.

\subsubsection{SDIMS}

SDIMS \cite{SDIMS} (Scalable Distributed Information Management System) proposes a combination of techniques employed in Astrolabe \cite{Renesse2003} and distributed hash tables (in this case, Pastry \cite{rowstron2001pastry}).

It is based on an abstraction which exposes the internal abstraction of aggregation trees provided by a DHT such as Pastry. Given a key $k$, an aggregation tree is defined by the the union of the routing paths from all nodes to key $k$, where each routing step along the path to $k$ corresponds to a level in the aggregation tree.

Then, aggregation functions are associated an attribute type and name, and rooted at \textit{hash(attribute type, attribute name)}, which results in different attributes with the same function being aggregated along trees rooted in different parts of the DHT, which enables load-balancing.

This achieves communication and memory efficiency when compared to gossip-based approaches, because MIBs have a lesser degree of replication, however, limitations which arise from employing SDIMS is that each node acts as an intermediate aggregation point for some attributes and as a leaf node for other attributes, which could potentially be a problem in the edge environment, given that low-capacity nodes may become overloaded if they are intermediate aggregation points in multiple aggregation trees. 

\subsubsection{Prometheus}

\textbf{Prometheus} \cite{prometheus} is an open-source monitoring and alerting toolkit originally built at SoundCloud. Prometheus works well for recording any purely numeric time series. It supports machine-centric monitoring as well as monitoring of highly dynamic service-oriented architectures. 

Prometheus is especially useful towards querying and collecting multi-dimensional data collections, it offers a platform towards configuring alerts, which are triggered whenever a certain criteria is met. 

It allows federation, which consists in a prometheus server scraping selected time-series from another Prometheus server. Federation is split in two categories, \textit{hierarchical federation} and \textit{cross-service federation}. In \textit{hierarchical federation}, prometheus servers are organized into a topology which resembles a tree, where each server aggregates aggregated time series data from a larger number of subordinated servers. Alternatively,  \textit{cross-service federation} enables scraping selected data from another service's prometheus server to enable alerting and queries against both datasets within a single server. 

Prometheus employs a variety of service discovery options for discovering and scraping targets. A key feature which improves its flexibility is the option to use file-based discovery, which enables the use of customized service discovery. This is achieved by programmers by providing a custom file with list of targets (and target metadata) which Prometheus will scrape.

\subsection{Discussion}

We believe there is a lack of monitoring systems targeted towards edge environments, in our study of the literature, we did not find many resources regarding monitoring large numbers of heterogenous nodes at a large scale. Most studied large scale information systems are targeted towards cloud environments, which as previously mentioned contrast immensely with the edge environment. These solutions often rely on techniques such as IP multicast \cite{massie2004ganglia} to perform efficiently.

Furthermore, we argue that large-scale monitoring systems purely based on distributed hash tables \cite{SDIMS} are unsuitable for edge environments, as  devices are heavily constrained in memory and often are unreliable routers (which a DHT assumes all nodes can reliably do). Conversely, pure gossip systems such as Astrolabe \cite{Renesse2003} require heavy amounts of message exchanges to keep information up-to-date, and require manual configuration of the hierarchical tree. 



%\subsection{End-to-end link monitoring}

%Given that the edge infrastructure envisions cooperation from all devices in the path from the origin of the data to the DC, devices need to be interconnected across an underlying infrastructure which is continuously changing. This raises concerns about the network quality of links between devices across the system, especially if they are running time-critical services. 

%It is paramount to analyze how to monitor and improve link quality, for providing traffic locality, latency, among others. According to the literature \cite{}, the most popular metrics to analyze are:

%\begin{enumerate}

    %\item Network throughput, which is the average rate of successful data transfer through a network connection.
    
    %\item Latency, which consists in how long a packet takes to travel across a link from one endpoint to another
    
    %\item Packet loss, which consists in how many packets are lost when traveling towards their destination.
%\end{enumerate}